{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 1 TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import depthai as dai\n",
    "\n",
    "\n",
    "DIM = (720, 480)\n",
    "\n",
    "# Closer-in minimum depth, disparity range is doubled (from 95 to 190):\n",
    "extended_disparity = False\n",
    "# Better accuracy for longer distance, fractional disparity 32-levels:\n",
    "subpixel = False\n",
    "# Better handling for occlusions:\n",
    "lr_check = True\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "camRgb = pipeline.create(dai.node.ColorCamera)\n",
    "camRgb.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "xoutRgb = pipeline.createXLinkOut()\n",
    "xoutRgb.setStreamName(\"rgb\")\n",
    "camRgb.video.link(xoutRgb.input)\n",
    "\n",
    "# Initialize the parameters\n",
    "confThreshold = 0.5  # Confidence threshold\n",
    "nmsThreshold = 0.4  # Non-maximum suppression threshold\n",
    "inpWidth = 256  # Width of network's input image\n",
    "inpHeight = 256  # Height of network's input image\n",
    "start = time.time()\n",
    "\n",
    "# Load names of classes\n",
    "classesFile =r\"E:\\GSU\\CV\\Assignment4\\coco.names\"\n",
    "classes = None\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')\n",
    "\n",
    "# modelConfiguration = \"tiny-yolov2-trial13.cfg\"\n",
    "# modelWeights = \"tiny-yolov2-trial13.weights\"\n",
    "\n",
    "modelConfiguration = r\"E:\\GSU\\CV\\Assignment4\\tiny.cfg\"\n",
    "modelWeights = r\"E:\\GSU\\CV\\Assignment4\\yolov3-tiny.weights\"\n",
    "\n",
    "# modelConfiguration = \"yolov3.cfg\"\n",
    "# modelWeights = \"yolov3.weights\"\n",
    "\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask, vertices, match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "# Get the names of the output layers\n",
    "def getOutputsNames(net):\n",
    "    # Get the names of all the layers in the network\n",
    "    layersNames = net.getLayerNames()\n",
    "    # Get the names of the output layers, i.e. the layers with unconnected outputs\n",
    "    outputlayers = [layersNames[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    return outputlayers\n",
    "\n",
    "\n",
    "# Remove the bounding boxes with low confidence using non-maxima suppression\n",
    "def postprocess(frame, outs):\n",
    "    frameHeight = frame.shape[0]\n",
    "    frameWidth = frame.shape[1]\n",
    "\n",
    "    # Scan through all the bounding boxes output from the network and keep only the\n",
    "    # ones with high confidence scores. Assign the box's class label as the class with the highest score.\n",
    "    classIds = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            if confidence > confThreshold:\n",
    "                center_x = int(detection[0] * frameWidth)\n",
    "                center_y = int(detection[1] * frameHeight)\n",
    "                width = int(detection[2] * frameWidth)\n",
    "                height = int(detection[3] * frameHeight)\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                classIds.append(classId)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
    "    # lower confidences.\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, confThreshold, nmsThreshold)\n",
    "    # print(indices)\n",
    "    left, top, width, height = 0, 0 , 0, 0\n",
    "    if len(indices) > 0:\n",
    "        for i in indices:\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]\n",
    "            cv2.putText(frame, classes[classIds[i]], (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, str(round(confidences[i] * 100, 2)) + \"%\", (left, top + height + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 3)\n",
    "\n",
    "\n",
    "\n",
    "# Process inputs\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "counter = 0\n",
    "time_elasped = 0\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Output queue will be used to get the disparity frames from the outputs defined above\n",
    "    qRgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    while cv2.waitKey(1) < 0:\n",
    "\n",
    "        # get frame from the video\n",
    "        # hasFrame, frame = cap.read()\n",
    "        inRgb = qRgb.get()\n",
    "        frame = inRgb.getCvFrame()\n",
    "        counter += 1\n",
    "\n",
    "        visualize = frame.copy()\n",
    "\n",
    "        # Create a 4D blob from a frame.\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1 / 255, (inpWidth, inpHeight), [0, 0, 0], 1, crop=False)\n",
    "\n",
    "        # Sets the input to the network\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Runs the forward pass to get output of the output layers\n",
    "        outs = net.forward(getOutputsNames(net))\n",
    "\n",
    "        # Remove the bounding boxes with low confidence\n",
    "        postprocess(frame, outs)\n",
    "        cv2.putText(frame, \"Q to Exit\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 4)\n",
    "        time_elasped = int(time.time() - start)\n",
    "        if time_elasped > 1:\n",
    "            cv2.putText(frame, \"FPS: \" + str(counter // time_elasped), (50, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2,\n",
    "                        (255, 255, 255), 3)\n",
    "            print(\"FPS: \", counter // time_elasped)\n",
    "\n",
    "        frame = cv2.resize(frame, (720, 480))\n",
    "\n",
    "        cv2.imshow(\"Object Detection YOLO\", frame)\n",
    "\n",
    "        # Stop the program if reached end of video\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            print(\"Done processing !!!\")\n",
    "            # cap.release()\n",
    "            end = time.time()\n",
    "            print(\"Time Elasped: \", int(end - start))\n",
    "            print(\"FPS: \", counter // (end - start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 2 BuSINESS CARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea is implemented using \n",
    "Image Detection - Detect an image\n",
    "Feature Matching - Match the Features using\n",
    "using the Augmented Reality displaying the image on the detected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MIN_MATCHES = 5\n",
    "detector = cv2.ORB_create(nfeatures=5000)\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=100)\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "\n",
    "def load_input(inp):\n",
    "\n",
    "\tinput_image = cv2.imread(r'E:\\GSU\\CV\\Assignment4\\Input\\img{0}.jpg'.format(inp))\n",
    "\taugment_image = cv2.imread(r'E:\\GSU\\CV\\Assignment4\\mask\\img{0}.jpg'.format(inp))\n",
    "\n",
    "\tinput_image = cv2.resize(input_image, (300,400),interpolation=cv2.INTER_AREA)\n",
    "\taugment_image = cv2.resize(augment_image, (300,400))\n",
    "\tgray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n",
    "\t# find the keypoints with ORB\n",
    "\tkeypoints, descriptors = detector.detectAndCompute(gray_image, None)\n",
    "\n",
    "\treturn gray_image,augment_image,keypoints, descriptors\n",
    "\n",
    "\n",
    "def compute_matches(descriptors_input, descriptors_output):\n",
    "\t# Match descriptors\n",
    "\tif(len(descriptors_output)!=0 and len(descriptors_input)!=0):\n",
    "\t\tmatches = flann.knnMatch(np.asarray(descriptors_input,np.float32),np.asarray(descriptors_output,np.float32),k=2)\n",
    "\t\tgood = []\n",
    "\t\tfor m,n in matches:\n",
    "\t\t\tif m.distance < 0.69*n.distance:\n",
    "\t\t\t\tgood.append(m)\n",
    "\t\treturn good\n",
    "\telse:\n",
    "\t\treturn None\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "\t#Getting Information form the Input image\n",
    "\tcount=1\n",
    "\tinput_image, aug_image, input_keypoints, input_descriptors = load_input(count)\n",
    "\tcap = cv2.VideoCapture(0)\n",
    "\tret, frame = cap.read()\n",
    "\twhile(ret):\n",
    "\t\tret, frame = cap.read()\n",
    "\t\tif(len(input_keypoints)<MIN_MATCHES):\n",
    "\t\t\tcontinue\n",
    "\t\tframe = cv2.resize(frame, (600,450))\n",
    "\t\tframe_bw = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t\toutput_keypoints, output_descriptors = detector.detectAndCompute(frame_bw, None)\n",
    "\t\t\n",
    "\t\tmatches = compute_matches(input_descriptors, output_descriptors)\n",
    "\t\tif(matches!=None):\n",
    "\t\t\tif(len(matches)>10):\n",
    "\t\t\t\tsrc_pts = np.float32([ input_keypoints[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "\t\t\t\tdst_pts = np.float32([ output_keypoints[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "\n",
    "\t\t\t\t#Finally find the homography matrix\n",
    "\t\t\t\tM, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "\t\t\t\tpts = np.float32([ [0,0],[0,399],[299,399],[299,0] ]).reshape(-1,1,2)\n",
    "\t\t\t\tdst = cv2.perspectiveTransform(pts,M)\n",
    "\t\t\t\tM_aug = cv2.warpPerspective(aug_image, M, (600,450))\n",
    "\n",
    "\t\t\t\t#getting the frame ready for addition operation with Mask Image\n",
    "\t\t\t\tframeb = cv2.fillConvexPoly(frame,dst.astype(int),0)\n",
    "\t\t\t\tFinal = frameb+M_aug\n",
    "\t\t\t\tcv2.imshow('Final Output', Final)\n",
    "\t\t\telse:\n",
    "\t\t\t\t\n",
    "\t\t\t\tcount+=1\n",
    "\t\t\t\t\n",
    "\t\t\t\tif count>2:\n",
    "\t\t\t\t\tcount=1\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tinput_image, aug_image, input_keypoints, input_descriptors=load_input(count)\n",
    "\t\t\t\tcv2.imshow('Final Output', frame)\n",
    "\t\telse:\n",
    "\t\t\tcv2.imshow('Final Output', frame)\n",
    "\t\tkey = cv2.waitKey(15)\n",
    "\t\tif(key==27):\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "241bc809ea0be1aa218bebbbf2bb0f01cfc910bda793a5a80212f290ab01833f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
